@article{
recall, 
title={Precision-Recall versus Accuracy and the Role of Large Data Sets},
volume={33},
url={https://ojs.aaai.org/index.php/AAAI/article/view/5193},
DOI={10.1609/aaai.v33i01.33014039}, 
abstractNote={&lt;p&gt;Practitioners of data mining and machine learning have long observed that the imbalance of classes in a data set negatively impacts the quality of classifiers trained on that data. Numerous techniques for coping with such imbalances have been proposed, but nearly all lack any theoretical grounding. By contrast, the standard theoretical analysis of machine learning admits no dependence on the imbalance of classes at all. The basic theorems of statistical learning establish the number of examples needed to estimate the accuracy of a classifier as a function of its complexity (VC-dimension) and the confidence desired; the class imbalance does not enter these formulas anywhere. In this work, we consider the measures of classifier performance in terms of precision and recall, a measure that is widely suggested as more appropriate to the classification of imbalanced data. We observe that whenever the precision is moderately large, the worse of the precision and recall is within a small constant factor of the accuracy weighted by the class imbalance. A corollary of this observation is that a larger number of examples is necessary and sufficient to address class imbalance, a finding we also illustrate empirically.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence},
author={Juba, Brendan and Le, Hai S.},
year={2019},
month={Jul.},
pages={4039-4048} 
}


@article{specificity,
author = {Glaros, Alan G. and Kline, Rex B.},
title = {Understanding the accuracy of tests with cutting scores: The sensitivity, specificity, and predictive value model},
journal = {Journal of Clinical Psychology},
volume = {44},
number = {6},
pages = {1013-1023},
doi = {https://doi.org/10.1002/1097-4679(198811)44:6<1013::AID-JCLP2270440627>3.0.CO;2-Z},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/1097-4679%28198811%2944%3A6%3C1013%3A%3AAID-JCLP2270440627%3E3.0.CO%3B2-Z},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/1097-4679%28198811%2944%3A6%3C1013%3A%3AAID-JCLP2270440627%3E3.0.CO%3B2-Z},
abstract = {Abstract While researchers usually are concerned about psychometric properties of psychological tests estimated using large samples, most clinical decisionmakers must evaluate the accuracy of test results for individuals. This is particularly true as regards tests that have cutting scores to determine, for example, whether to assign a particular diagnosis or accept an applicant into a training program. This paper reviews a conceptual model that may foster improved understanding of test outcomes for individuals. The terms “sensitivity,” “specificity,” and “predictive value” are defined, and the relations of positive and negative predictive values to population base rates are emphasized. Examples from the psychological literature are presente to illustrate the utility of these concepts in clinical decision-making with psychological tests. Implications for test users, test developers, and instructors are discussed.},
year = {1988}
}


@article{accuracy,
  title={Using AUC and accuracy in evaluating learning algorithms},
  author={Huang, Jin and Ling, Charles X},
  journal={IEEE Transactions on knowledge and Data Engineering},
  volume={17},
  number={3},
  pages={299--310},
  year={2005},
  publisher={IEEE}
}
